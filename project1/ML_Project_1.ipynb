{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888ab943",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87713261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b2d85",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8acbb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some helper functions for project 1.\"\"\"\n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y == \"b\")] = -1\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in .csv format for submission to Kaggle or AIcrowd\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, \"w\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1), \"Prediction\": int(r2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a215ce",
   "metadata": {},
   "source": [
    "# Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436f9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTOL=1e-4\n",
    "ATOL=1e-8\n",
    "\n",
    "MAX_ITERS = 2\n",
    "GAMMA = 0.1\n",
    "\n",
    "def initial_w_testing():\n",
    "    return np.array([[0.5], [1.0]])\n",
    "\n",
    "def y_testing():\n",
    "    return np.array([[0.1], [0.3], [0.5]])\n",
    "\n",
    "def tx_testing():\n",
    "    return np.array([[2.3, 3.2], [1.0, 0.1], [1.4, 2.3]])\n",
    "\n",
    "def test_least_squares(y, tx):\n",
    "    w, loss = least_squares(y, tx)\n",
    "\n",
    "    expected_w = np.array([[0.218786], [-0.053837]])\n",
    "    expected_loss = 0.026942\n",
    "\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_ridge_regression_lambda0(y, tx):\n",
    "    lambda_ = 0.0\n",
    "    w, loss = ridge_regression(y, tx, lambda_)\n",
    "\n",
    "    expected_loss = 0.026942\n",
    "    expected_w = np.array([[0.218786], [-0.053837]])\n",
    "\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_ridge_regression_lambda1(y, tx):\n",
    "    lambda_ = 1.0\n",
    "    w, loss = ridge_regression(y, tx, lambda_)\n",
    "\n",
    "    expected_loss = 0.03175\n",
    "    expected_w = np.array([[0.054303], [0.042713]])\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_logistic_regression_0_step(y, tx):\n",
    "    expected_w = np.array([[0.463156], [0.939874]])\n",
    "    y = (y > 0.2) * 1.0\n",
    "    w, loss = logistic_regression(y, tx, expected_w, 0, GAMMA)\n",
    "\n",
    "    expected_loss = 1.533694\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_logistic_regression(y, tx, initial_w):\n",
    "    y = (y > 0.2) * 1.0\n",
    "    w, loss = logistic_regression(\n",
    "        y, tx, initial_w, MAX_ITERS, GAMMA\n",
    "    )\n",
    "\n",
    "    expected_loss = 1.348358\n",
    "    expected_w = np.array([[0.378561], [0.801131]])\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "    \n",
    "def test_reg_logistic_regression(y, tx, initial_w):\n",
    "    lambda_ = 1.0\n",
    "    y = (y > 0.2) * 1.0\n",
    "    w, loss = reg_logistic_regression(\n",
    "        y, tx, lambda_, initial_w, MAX_ITERS, GAMMA\n",
    "    )\n",
    "\n",
    "    expected_loss = 0.972165\n",
    "    expected_w = np.array([[0.216062], [0.467747]])\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_reg_logistic_regression_0_step(y, tx):\n",
    "    lambda_ = 1.0\n",
    "    expected_w = np.array([[0.409111], [0.843996]])\n",
    "    y = (y > 0.2) * 1.0\n",
    "    w, loss = reg_logistic_regression(\n",
    "        y, tx, lambda_, expected_w, 0, GAMMA\n",
    "    )\n",
    "\n",
    "    expected_loss = 1.407327\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1717d4c",
   "metadata": {},
   "source": [
    "# Our Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb8ef1",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7677a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        w: weights, numpy array of shape(D,), D is the number of features.\n",
    "    \n",
    "    Returns:\n",
    "        mse: scalar corresponding to the mse with factor (1 / 2 n) in front of the sum\n",
    "\n",
    "    >>> compute_mse(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), np.array([0.03947092, 0.00319628]))\n",
    "    0.006417022764962313\n",
    "    \"\"\"\n",
    "    \n",
    "    e = y - tx.dot(w)\n",
    "    \n",
    "    ## np.linalg.norm(e) ** 2 replaces e.dot(e)\n",
    "    mse = (np.linalg.norm(e) ** 2) / (2 * len(e))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52ed2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        mse: scalar.\n",
    "    \"\"\"\n",
    "    # w = (np.linalg.inv(tx.transpose() @ tx)) @ (tx.transpose() @ y)\n",
    "    w = np.linalg.solve(tx.transpose() @ tx, tx.transpose() @ y)\n",
    "    mse = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_least_squares(y_testing(), tx_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0d90d",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c2a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "    \"\"\"\n",
    "    D = 1 if len(tx.shape) == 1 else tx.shape[1]\n",
    "    N = len(y)\n",
    "    a = np.array((tx.transpose() @ tx) + (2 * N * lambda_) * np.eye(D))\n",
    "    b = np.array(tx.transpose() @ y)\n",
    "    \n",
    "    w = np.linalg.inv(a) @ b\n",
    "        \n",
    "    return w, compute_mse(y, tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0910a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ridge_regression_lambda0(y_testing(), tx_testing())\n",
    "test_ridge_regression_lambda1(y_testing(), tx_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827c388",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7acd0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Passes the tests! ^^\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\n",
    "\n",
    "    Args:\n",
    "        t: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "        scalar or numpy array\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "\n",
    "    Returns:\n",
    "        a hessian matrix of shape=(D, D) \n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(y)\n",
    "    sig = sigmoid(tx @ w)\n",
    "    diag = np.zeros((N, N))\n",
    "    np.fill_diagonal(diag,  sig * (1 - sig))\n",
    "                  \n",
    "    return (1 / N) * ((tx.T @ diag) @ tx)\n",
    "\n",
    "def calculate_loss(y, tx, w, penalty=0):\n",
    "    \"\"\"compute the cost by negative log likelihood.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "\n",
    "    Returns:\n",
    "        a non-negative loss\n",
    "    \"\"\"\n",
    "    assert y.shape[0]  == tx.shape[0]\n",
    "    assert tx.shape[1] == w.shape[0]\n",
    "    \n",
    "    sig = sigmoid(tx @ w) \n",
    "    left  = y * np.log(sig)\n",
    "    right = (1-y) * np.log(1 - sig)\n",
    "    \n",
    "    return - np.mean(left + right) + penalty\n",
    "\n",
    "def calculate_gradient(y, tx, w, lambda_=0):\n",
    "    \"\"\"compute the gradient of loss.\n",
    "    \n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "\n",
    "    Returns:\n",
    "        a vector of shape (D, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    sig = sigmoid(tx @ w)\n",
    "    \n",
    "    ## last term is for adding a lambda_ * ||w||^2 penalty \n",
    "    return (1 / len(y)) * (tx.T @ (sig - y)) + (2 * lambda_ * w)\n",
    "\n",
    "## Uses gradient descent\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression with GD\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss = calculate_loss(y, tx, w) \n",
    "        w = w - gamma * calculate_gradient(y, tx, w)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    losses.append(calculate_loss(y, tx, w))\n",
    "    print(\"loss={l}\".format(l=losses[-1]))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3dd653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.5336935954900122\n",
      "Current iteration=0, loss=1.6162998438550205\n",
      "loss=1.348357817512931\n"
     ]
    }
   ],
   "source": [
    "test_logistic_regression_0_step(y_testing(), tx_testing())\n",
    "test_logistic_regression(y_testing(), tx_testing(), initial_w_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0e102",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38baf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_penalty_term(lambda_, w):\n",
    "    return lambda_ * (np.linalg.norm(w) ** 2)\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression with GD\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss = calculate_loss(y, tx, w, penalty=compute_penalty_term(lambda_, w)) \n",
    "        w = w - gamma * calculate_gradient(y, tx, w, lambda_=lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    losses.append(calculate_loss(y, tx, w))\n",
    "    print(\"loss={l}\".format(l=losses[-1]))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2a32bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.4073273893870055\n",
      "Current iteration=0, loss=2.8662998438550207\n",
      "loss=0.9721649929512527\n"
     ]
    }
   ],
   "source": [
    "test_reg_logistic_regression_0_step(y_testing(), tx_testing())\n",
    "test_reg_logistic_regression(y_testing(), tx_testing(), initial_w_testing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f84a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14a3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353f06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "015c6520",
   "metadata": {},
   "source": [
    "# Loading Higgs Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71237916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-2dcf493d136b>:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  ids = x[:, 0].astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "y_test,  tx_test,  ids_test  = load_csv_data(\"D:/Downloads/test.csv\")\n",
    "y_train, tx_train, ids_train = load_csv_data(\"D:/Downloads/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bbdf069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 250000\n",
      "Number of features: 30\n"
     ]
    }
   ],
   "source": [
    "N, D = tx_train.shape\n",
    "\n",
    "print(f'Number of samples: {N}')\n",
    "print(f'Number of features: {D}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70c76100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7c8644c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = np.arange(0, 250000, 1000)\n",
    "\n",
    "xx = tx_train.T[0].T[indices]\n",
    "\n",
    "yy = y_train[indices]\n",
    "\n",
    "plt.scatter(xx, yy, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620faa4f",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3529711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(x):\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dd83e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_train = standardize_data(tx_train)\n",
    "tx_test  = standardize_data(tx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91b24b",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db7cc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = len(y_test)\n",
    "\n",
    "y_test = np.reshape(y_test, (N_test, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da95ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.693147180559945\n",
      "Current iteration=100, loss=0.15701615039582964\n",
      "loss=0.09618928247245291\n",
      "Test loss: 1.4844257445772375\n"
     ]
    }
   ],
   "source": [
    "N, D = tx_train.shape\n",
    "\n",
    "y_train = np.reshape(y_train, (N, 1))\n",
    "max_iter = 150\n",
    "gamma = 0.1\n",
    "initial_weights = np.zeros((D, 1))\n",
    "\n",
    "weights, loss = logistic_regression(y_train, tx_train, initial_weights, max_iter, gamma)\n",
    "\n",
    "print(f'Test loss: {calculate_loss(y_test, tx_test, weights)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "254ffcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4844257445772375\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7001fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dunno how to visualize this\n",
    "\n",
    "## Other ideas: hyperparameter search for gamma; OR gamma function that decreases over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7a0db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Logistic Regression] In max_iter=150, with hyperparameter gamma=0.1 we obtain a loss=0.09618928247245291\n"
     ]
    }
   ],
   "source": [
    "print(f'[Logistic Regression] In max_iter={max_iter}, with hyperparameter gamma={gamma} we obtain a loss={loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872a46e",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881d34d",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "    \n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,D)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, D = x.shape\n",
    "\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train\n",
    "    # ***************************************************\n",
    "    k_te_indices = k_indices[k]\n",
    "    te_mask = np.zeros(N, dtype = bool)\n",
    "    te_mask[k_te_indices] = True\n",
    "    \n",
    "    y_te = y[te_mask]\n",
    "    y_tr = y[~te_mask]\n",
    "    \n",
    "    x_te = x[te_mask]\n",
    "    x_tr = x[~te_mask]\n",
    "    \n",
    "    # ***************************************************\n",
    "    # Regularized logistic regression\n",
    "    # ***************************************************\n",
    "    \n",
    "    initial_weights = np.zeros((D, 1))\n",
    "    gamma = 0.1\n",
    "    max_iters = 150\n",
    "    weights, loss = reg_logistic_regression(y_tr, x_tr, lambda_, initial_weights, max_iters, gamma)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data\n",
    "    # ***************************************************\n",
    "    \n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_tr, x_tr, weights))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_te, x_te, weights))\n",
    "    \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "def cross_validation_demo(y, x, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "    \n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    seed = 12\n",
    "    k_fold = k_fold\n",
    "    lambdas = lambdas\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # cross validation over lambdas\n",
    "    # ***************************************************\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        aux_tr = 0; aux_te = 0\n",
    "        for k in np.arange(k_fold):\n",
    "            loss_tr_tmp, loss_te_tmp = cross_validation(y, x, k_indices, k, lambda_)\n",
    "            \n",
    "            aux_tr += loss_tr_tmp\n",
    "            aux_te += loss_te_tmp\n",
    "            \n",
    "        rmse_tr.append(aux_tr/k_fold)\n",
    "        rmse_te.append(aux_te/k_fold)   \n",
    "\n",
    "    ## Computing the best lambda & test rmse tuple\n",
    "    best_idx = np.argmin(rmse_te)\n",
    "    \n",
    "    best_lambda = lambdas[best_idx]\n",
    "    best_rmse   = rmse_te[best_idx]\n",
    "        \n",
    "    print(\"The choice of lambda which leads to the best test rmse is %.5f with a test rmse of %.3f\" % (best_lambda, best_rmse))\n",
    "    return best_lambda, best_rmse\n",
    "\n",
    "##Fn call\n",
    "best_lambda, best_rmse = cross_validation_demo(y_train, tx_train, 4, np.logspace(-4, 0, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_param_selection(y, x, gammas, k_fold, lambdas, seed = 1):\n",
    "    \"\"\"cross validation over regularisation parameter lambda and gradient descent step gamma.\n",
    "    \n",
    "    Args:\n",
    "        gammas: shape = (d,), where d is the number of values of gamma to test \n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_gamma  : scalar, value of the best gamma\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : value of the rmse for the couple (best_gamma, best_lambda)\n",
    "    \"\"\"\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # cross validation over degrees and lambdas\n",
    "    # ***************************************************\n",
    "    \n",
    "    rmse_matrix_tr = []\n",
    "    rmse_matrix_te = []\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr = []; rmse_te = []\n",
    "        \n",
    "        for degree in degrees:\n",
    "            aux_tr = 0; aux_te = 0\n",
    "            \n",
    "            for k in np.arange(k_fold):\n",
    "                loss_tr_tmp, loss_te_tmp = cross_validation(y, x, k_indices, k, lambda_, gamma)\n",
    "            \n",
    "                aux_tr += loss_tr_tmp\n",
    "                aux_te += loss_te_tmp\n",
    "            ##end for-k\n",
    "            rmse_tr.append(aux_tr/k_fold)\n",
    "            rmse_te.append(aux_te/k_fold)\n",
    "        ##end for-deg\n",
    "        rmse_matrix_tr.append(rmse_tr)\n",
    "        rmse_matrix_te.append(rmse_te)\n",
    "    ##end for-lambda\n",
    "    \n",
    "    best_idx = np.argmin(rmse_matrix_te)\n",
    "    \n",
    "    l_idx = best_idx // len(lambdas)\n",
    "    d_idx = best_idx % len(degrees)\n",
    "    \n",
    "    best_lambda = lambdas[l_idx]\n",
    "    best_degree = degrees[d_idx]\n",
    "    best_rmse = rmse_matrix_te[l_idx][d_idx]\n",
    "        \n",
    "    return best_degree, best_lambda, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb1f425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.693147180559945\n",
      "Current iteration=100, loss=0.33620430535469925\n",
      "loss=0.22785590913974219\n",
      "1.0748463695319812\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.05\n",
    "initial_weight = np.zeros((D,1))\n",
    "gamma = 0.1\n",
    "max_iter = 150\n",
    "\n",
    "weights, loss = reg_logistic_regression(y_train, tx_train, lambda_, initial_weight, max_iter, gamma)\n",
    "\n",
    "print(calculate_loss(y_test, tx_test, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa83826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4771610607730876\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "258494a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reg. Logistic Regression] In max_iter=150, with hyperparameters lambda=0.0005 and gamma=0.1 we obtain a loss=0.09789751369434345 and a test loss=5 TODO\n"
     ]
    }
   ],
   "source": [
    "print(f'[Reg. Logistic Regression] In max_iter={max_iter}, with hyperparameters lambda={lambda_} and gamma={gamma} we obtain a loss={loss} and a test loss={5} TODO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
