{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888ab943",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87713261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a215ce",
   "metadata": {},
   "source": [
    "# Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe00faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import ALL_TESTS, y_testing, tx_testing, initial_w_testing\n",
    "\n",
    "for test in ALL_TESTS:\n",
    "    try:\n",
    "        test(y_testing(), tx_testing())\n",
    "    except TypeError:\n",
    "        test(y_testing(), tx_testing(), initial_w_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c6520",
   "metadata": {},
   "source": [
    "# Loading Higgs Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71237916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA = Path().resolve() / \"data\"\n",
    "print(\"Looking for the data in\", DATA)\n",
    "y_test,  tx_test,  ids_test  = load_csv_data(DATA / \"test.csv\")\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdf069",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = tx_train.shape\n",
    "\n",
    "print(f'Number of samples: {N}')\n",
    "print(f'Number of features: {D}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88766c74",
   "metadata": {},
   "source": [
    "# Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize each feature\n",
    "def normalize_features(x):\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "tx_normalised = normalize_features(tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_every_feature(tx):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.boxplot(tx)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Deviation from the mean')\n",
    "    plt.title(\"Boxplot of each feature\")\n",
    "    plt.show()\n",
    "\n",
    "boxplot_every_feature(tx_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_each_feature(x, y):\n",
    "    \"\"\"Plot the density distribution of every feature in one plot.\n",
    "    The categories are represented by different colors.\"\"\"\n",
    "    size = int(np.ceil(np.sqrt(x.shape[1])))\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    for i in range(x.shape[1]):\n",
    "        plt.subplot(size, size, i+1)\n",
    "        plt.hist(x[y == -1, i], bins=50, alpha=0.5, label='-1')\n",
    "        plt.hist(x[y == 1, i], bins=50, alpha=0.5, label='1')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Feature {i}')\n",
    "    plt.show()\n",
    "plot_each_feature(tx_normalised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdness = np.sum(np.abs(tx_normalised), axis=1)\n",
    "# histogram of weirdness\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(weirdness, bins=100)\n",
    "plt.xlabel('sum of deviations from the mean')\n",
    "plt.ylabel('number of samples')\n",
    "plt.title('Histogram of weirdness')\n",
    "plt.show()\n",
    "\n",
    "WEIRDNESS_THRESHOLD = 60\n",
    "print(f'Number of samples with weirdness > {WEIRDNESS_THRESHOLD}: {np.sum(weirdness > WEIRDNESS_THRESHOLD)}')\n",
    "\n",
    "tx_cleaner = tx_normalised[weirdness < WEIRDNESS_THRESHOLD]\n",
    "y_cleaner = y_train[weirdness < WEIRDNESS_THRESHOLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebcdda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_cleaner = normalize_features(tx_cleaner)\n",
    "\n",
    "boxplot_every_feature(tx_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b8cf10",
   "metadata": {},
   "source": [
    "### We now look into the correlations between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation between every pair of features\n",
    "correlation_matrix = np.corrcoef(tx_cleaner.T)\n",
    "\n",
    "# plot the correlation matrix\n",
    "plt.figure()\n",
    "plt.imshow(correlation_matrix, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Feature correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d96dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect the correlation between strongly correlated features\n",
    "# (i.e. features with a correlation coefficient > 0.9)\n",
    "from math import ceil\n",
    "\n",
    "COR_THRESHOLD = 0.95  # or 0.99\n",
    "strong_correlations = np.where(np.abs(correlation_matrix) > 0.95)\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "indices = np.random.randint(0, tx_cleaner.shape[0], 1000)\n",
    "nb_correlations = len(strong_correlations[0]) - D\n",
    "nb = 0\n",
    "\n",
    "groups = []\n",
    "for i, j in zip(*strong_correlations):\n",
    "    if i < j:\n",
    "        for group in groups:\n",
    "            if i in group or j in group:\n",
    "                group.add(i)\n",
    "                group.add(j)\n",
    "                break\n",
    "        else:\n",
    "            groups.append({i, j})\n",
    "\n",
    "        print(f'Correlation between feature {i} and feature {j}: {correlation_matrix[i, j]}')\n",
    "        plt.subplot(ceil(nb_correlations / 4), 4, nb + 1)\n",
    "        plt.scatter(tx_cleaner[indices, i], tx_cleaner[indices, j], c=y_cleaner[indices])\n",
    "        plt.xlabel(f'Feature {i}')\n",
    "        plt.ylabel(f'Feature {j}')\n",
    "        plt.title(f'Correlation between {i} and {j}: {correlation_matrix[i, j]}')\n",
    "        nb += 1\n",
    "plt.show()\n",
    "\n",
    "print(f'Number of groups of strongly correlated features: {len(groups)}')\n",
    "print(f'Number of strongly correlated features: {sum(len(group) for group in groups)}')\n",
    "print(f'Groups of strongly correlated features: {groups}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the features that are strongly correlated together, \n",
    "# as they don't hold any additional information\n",
    "to_delete = []\n",
    "for group in groups:\n",
    "    to_delete.extend(list(group)[1:])\n",
    "print(f'Deleting features {to_delete}')\n",
    "tx_clean = np.delete(tx_cleaner, to_delete, axis=1)\n",
    "\n",
    "# Print the new correlation matrix\n",
    "correlation_matrix = np.corrcoef(tx_clean.T)\n",
    "plt.figure()\n",
    "plt.imshow(correlation_matrix, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Feature correlation matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620faa4f",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"Split the dataset between train and test based on the split ratio.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n = len(y)\n",
    "    indices = np.random.permutation(n)\n",
    "    split = int(ratio * n)\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return x[train_indices], y[train_indices], x[test_indices], y[test_indices]\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(tx_clean, y_cleaner, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d104cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squarred_error_sgd(y_train, x_train, np.randn(x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91b24b",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = len(y_test)\n",
    "\n",
    "y_test = np.reshape(y_test, (N_test, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = tx_train.shape\n",
    "\n",
    "y_train = np.reshape(y_train, (N, 1))\n",
    "max_iter = 150\n",
    "gamma = 0.1\n",
    "initial_weights = np.zeros((D, 1))\n",
    "\n",
    "weights, loss = logistic_regression(y_train, tx_train, initial_weights, max_iter, gamma)\n",
    "\n",
    "print(f'Test loss: {calculate_loss(y_test, tx_test, weights)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ffcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dunno how to visualize this\n",
    "\n",
    "## Other ideas: hyperparameter search for gamma; OR gamma function that decreases over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[Logistic Regression] In max_iter={max_iter}, with hyperparameter gamma={gamma} we obtain a loss={loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872a46e",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881d34d",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "    \n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,D)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, D = x.shape\n",
    "\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train\n",
    "    # ***************************************************\n",
    "    k_te_indices = k_indices[k]\n",
    "    te_mask = np.zeros(N, dtype = bool)\n",
    "    te_mask[k_te_indices] = True\n",
    "    \n",
    "    y_te = y[te_mask]\n",
    "    y_tr = y[~te_mask]\n",
    "    \n",
    "    x_te = x[te_mask]\n",
    "    x_tr = x[~te_mask]\n",
    "    \n",
    "    # ***************************************************\n",
    "    # Regularized logistic regression\n",
    "    # ***************************************************\n",
    "    \n",
    "    initial_weights = np.zeros((D, 1))\n",
    "    gamma = 0.1\n",
    "    max_iters = 150\n",
    "    weights, loss = reg_logistic_regression(y_tr, x_tr, lambda_, initial_weights, max_iters, gamma)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data\n",
    "    # ***************************************************\n",
    "    \n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_tr, x_tr, weights))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_te, x_te, weights))\n",
    "    \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "def cross_validation_demo(y, x, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "    \n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    seed = 12\n",
    "    k_fold = k_fold\n",
    "    lambdas = lambdas\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # cross validation over lambdas\n",
    "    # ***************************************************\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        aux_tr = 0; aux_te = 0\n",
    "        for k in np.arange(k_fold):\n",
    "            loss_tr_tmp, loss_te_tmp = cross_validation(y, x, k_indices, k, lambda_)\n",
    "            \n",
    "            aux_tr += loss_tr_tmp\n",
    "            aux_te += loss_te_tmp\n",
    "            \n",
    "        rmse_tr.append(aux_tr/k_fold)\n",
    "        rmse_te.append(aux_te/k_fold)   \n",
    "\n",
    "    ## Computing the best lambda & test rmse tuple\n",
    "    best_idx = np.argmin(rmse_te)\n",
    "    \n",
    "    best_lambda = lambdas[best_idx]\n",
    "    best_rmse   = rmse_te[best_idx]\n",
    "        \n",
    "    print(\"The choice of lambda which leads to the best test rmse is %.5f with a test rmse of %.3f\" % (best_lambda, best_rmse))\n",
    "    return best_lambda, best_rmse\n",
    "\n",
    "##Fn call\n",
    "best_lambda, best_rmse = cross_validation_demo(y_train, tx_train, 4, np.logspace(-4, 0, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_param_selection(y, x, gammas, k_fold, lambdas, seed = 1):\n",
    "    \"\"\"cross validation over regularisation parameter lambda and gradient descent step gamma.\n",
    "    \n",
    "    Args:\n",
    "        gammas: shape = (d,), where d is the number of values of gamma to test \n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_gamma  : scalar, value of the best gamma\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : value of the rmse for the couple (best_gamma, best_lambda)\n",
    "    \"\"\"\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # cross validation over degrees and lambdas\n",
    "    # ***************************************************\n",
    "    \n",
    "    rmse_matrix_tr = []\n",
    "    rmse_matrix_te = []\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr = []; rmse_te = []\n",
    "        \n",
    "        for degree in degrees:\n",
    "            aux_tr = 0; aux_te = 0\n",
    "            \n",
    "            for k in np.arange(k_fold):\n",
    "                loss_tr_tmp, loss_te_tmp = cross_validation(y, x, k_indices, k, lambda_, gamma)\n",
    "            \n",
    "                aux_tr += loss_tr_tmp\n",
    "                aux_te += loss_te_tmp\n",
    "            ##end for-k\n",
    "            rmse_tr.append(aux_tr/k_fold)\n",
    "            rmse_te.append(aux_te/k_fold)\n",
    "        ##end for-deg\n",
    "        rmse_matrix_tr.append(rmse_tr)\n",
    "        rmse_matrix_te.append(rmse_te)\n",
    "    ##end for-lambda\n",
    "    \n",
    "    best_idx = np.argmin(rmse_matrix_te)\n",
    "    \n",
    "    l_idx = best_idx // len(lambdas)\n",
    "    d_idx = best_idx % len(degrees)\n",
    "    \n",
    "    best_lambda = lambdas[l_idx]\n",
    "    best_degree = degrees[d_idx]\n",
    "    best_rmse = rmse_matrix_te[l_idx][d_idx]\n",
    "        \n",
    "    return best_degree, best_lambda, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.05\n",
    "initial_weight = np.zeros((D,1))\n",
    "gamma = 0.1\n",
    "max_iter = 150\n",
    "\n",
    "weights, loss = reg_logistic_regression(y_train, tx_train, lambda_, initial_weight, max_iter, gamma)\n",
    "\n",
    "print(calculate_loss(y_test, tx_test, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83826f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258494a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[Reg. Logistic Regression] In max_iter={max_iter}, with hyperparameters lambda={lambda_} and gamma={gamma} we obtain a loss={loss} and a test loss={5} TODO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
