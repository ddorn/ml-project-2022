{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888ab943",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87713261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a215ce",
   "metadata": {},
   "source": [
    "# Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe00faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests import ALL_TESTS, y_testing, tx_testing, initial_w_testing\n",
    "\n",
    "for test in ALL_TESTS:\n",
    "    try:\n",
    "        test(y_testing(), tx_testing())\n",
    "    except TypeError:\n",
    "        test(y_testing(), tx_testing(), initial_w_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c6520",
   "metadata": {},
   "source": [
    "# Loading Higgs Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71237916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA = Path().resolve() / \"data\"\n",
    "print(\"Looking for the data in\", DATA)\n",
    "y_test,  tx_test,  ids_test  = load_csv_data(DATA / \"test.csv\")\n",
    "y_train, tx_train, ids_train = load_csv_data(DATA / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdf069",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = tx_train.shape\n",
    "\n",
    "print(f'Number of samples: {N}')\n",
    "print(f'Number of features: {D}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(0, 250000, 1000)\n",
    "\n",
    "xx = tx_train.T[0].T[indices]\n",
    "\n",
    "yy = y_train[indices]\n",
    "\n",
    "plt.scatter(xx, yy, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620faa4f",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3529711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(x):\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd83e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_train = standardize_data(tx_train)\n",
    "tx_test  = standardize_data(tx_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91b24b",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = len(y_test)\n",
    "\n",
    "y_test = np.reshape(y_test, (N_test, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D = tx_train.shape\n",
    "\n",
    "y_train = np.reshape(y_train, (N, 1))\n",
    "max_iter = 150\n",
    "gamma = 0.1\n",
    "initial_weights = np.zeros((D, 1))\n",
    "\n",
    "weights, loss = logistic_regression(y_train, tx_train, initial_weights, max_iter, gamma)\n",
    "\n",
    "print(f'Test loss: {calculate_loss(y_test, tx_test, weights)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ffcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dunno how to visualize this\n",
    "\n",
    "## Other ideas: hyperparameter search for gamma; OR gamma function that decreases over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[Logistic Regression] In max_iter={max_iter}, with hyperparameter gamma={gamma} we obtain a loss={loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872a46e",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881d34d",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "    \n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,D)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, D = x.shape\n",
    "\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train\n",
    "    # ***************************************************\n",
    "    k_te_indices = k_indices[k]\n",
    "    te_mask = np.zeros(N, dtype = bool)\n",
    "    te_mask[k_te_indices] = True\n",
    "    \n",
    "    y_te = y[te_mask]\n",
    "    y_tr = y[~te_mask]\n",
    "    \n",
    "    x_te = x[te_mask]\n",
    "    x_tr = x[~te_mask]\n",
    "    \n",
    "    # ***************************************************\n",
    "    # Regularized logistic regression\n",
    "    # ***************************************************\n",
    "    \n",
    "    initial_weights = np.zeros((D, 1))\n",
    "    gamma = 0.1\n",
    "    max_iters = 150\n",
    "    weights, loss = reg_logistic_regression(y_tr, x_tr, lambda_, initial_weights, max_iters, gamma)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data\n",
    "    # ***************************************************\n",
    "    \n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_tr, x_tr, weights))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_te, x_te, weights))\n",
    "    \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "def cross_validation_demo(y, x, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "    \n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    seed = 12\n",
    "    k_fold = k_fold\n",
    "    lambdas = lambdas\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # cross validation over lambdas\n",
    "    # ***************************************************\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        aux_tr = 0; aux_te = 0\n",
    "        for k in np.arange(k_fold):\n",
    "            loss_tr_tmp, loss_te_tmp = cross_validation(y, x, k_indices, k, lambda_)\n",
    "            \n",
    "            aux_tr += loss_tr_tmp\n",
    "            aux_te += loss_te_tmp\n",
    "            \n",
    "        rmse_tr.append(aux_tr/k_fold)\n",
    "        rmse_te.append(aux_te/k_fold)   \n",
    "\n",
    "    ## Computing the best lambda & test rmse tuple\n",
    "    best_idx = np.argmin(rmse_te)\n",
    "    \n",
    "    best_lambda = lambdas[best_idx]\n",
    "    best_rmse   = rmse_te[best_idx]\n",
    "        \n",
    "    print(\"The choice of lambda which leads to the best test rmse is %.5f with a test rmse of %.3f\" % (best_lambda, best_rmse))\n",
    "    return best_lambda, best_rmse\n",
    "\n",
    "##Fn call\n",
    "best_lambda, best_rmse = cross_validation_demo(y_train, tx_train, 4, np.logspace(-4, 0, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_param_selection(y, x, gammas, k_fold, lambdas, seed = 1):\n",
    "    \"\"\"cross validation over regularisation parameter lambda and gradient descent step gamma.\n",
    "    \n",
    "    Args:\n",
    "        gammas: shape = (d,), where d is the number of values of gamma to test \n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_gamma  : scalar, value of the best gamma\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : value of the rmse for the couple (best_gamma, best_lambda)\n",
    "    \"\"\"\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # cross validation over degrees and lambdas\n",
    "    # ***************************************************\n",
    "    \n",
    "    rmse_matrix_tr = []\n",
    "    rmse_matrix_te = []\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr = []; rmse_te = []\n",
    "        \n",
    "        for degree in degrees:\n",
    "            aux_tr = 0; aux_te = 0\n",
    "            \n",
    "            for k in np.arange(k_fold):\n",
    "                loss_tr_tmp, loss_te_tmp = cross_validation(y, x, k_indices, k, lambda_, gamma)\n",
    "            \n",
    "                aux_tr += loss_tr_tmp\n",
    "                aux_te += loss_te_tmp\n",
    "            ##end for-k\n",
    "            rmse_tr.append(aux_tr/k_fold)\n",
    "            rmse_te.append(aux_te/k_fold)\n",
    "        ##end for-deg\n",
    "        rmse_matrix_tr.append(rmse_tr)\n",
    "        rmse_matrix_te.append(rmse_te)\n",
    "    ##end for-lambda\n",
    "    \n",
    "    best_idx = np.argmin(rmse_matrix_te)\n",
    "    \n",
    "    l_idx = best_idx // len(lambdas)\n",
    "    d_idx = best_idx % len(degrees)\n",
    "    \n",
    "    best_lambda = lambdas[l_idx]\n",
    "    best_degree = degrees[d_idx]\n",
    "    best_rmse = rmse_matrix_te[l_idx][d_idx]\n",
    "        \n",
    "    return best_degree, best_lambda, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.05\n",
    "initial_weight = np.zeros((D,1))\n",
    "gamma = 0.1\n",
    "max_iter = 150\n",
    "\n",
    "weights, loss = reg_logistic_regression(y_train, tx_train, lambda_, initial_weight, max_iter, gamma)\n",
    "\n",
    "print(calculate_loss(y_test, tx_test, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83826f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258494a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[Reg. Logistic Regression] In max_iter={max_iter}, with hyperparameters lambda={lambda_} and gamma={gamma} we obtain a loss={loss} and a test loss={5} TODO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
