{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888ab943",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87713261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b2d85",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acbb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some helper functions for project 1.\"\"\"\n",
    "def load_csv_data(data_path, sub_sample=False):\n",
    "    \"\"\"Loads data and returns y (class labels), tX (features) and ids (event ids)\"\"\"\n",
    "    y = np.genfromtxt(data_path, delimiter=\",\", skip_header=1, dtype=str, usecols=1)\n",
    "    x = np.genfromtxt(data_path, delimiter=\",\", skip_header=1)\n",
    "    ids = x[:, 0].astype(np.int)\n",
    "    input_data = x[:, 2:]\n",
    "\n",
    "    # convert class labels from strings to binary (-1,1)\n",
    "    yb = np.ones(len(y))\n",
    "    yb[np.where(y == \"b\")] = -1\n",
    "\n",
    "    # sub-sample\n",
    "    if sub_sample:\n",
    "        yb = yb[::50]\n",
    "        input_data = input_data[::50]\n",
    "        ids = ids[::50]\n",
    "\n",
    "    return yb, input_data, ids\n",
    "\n",
    "\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in .csv format for submission to Kaggle or AIcrowd\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, \"w\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1), \"Prediction\": int(r2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a215ce",
   "metadata": {},
   "source": [
    "# Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "RTOL=1e-4\n",
    "ATOL=1e-8\n",
    "\n",
    "MAX_ITERS = 2\n",
    "GAMMA = 0.1\n",
    "\n",
    "def initial_w_testing():\n",
    "    return np.array([[0.5], [1.0]])\n",
    "\n",
    "def y_testing():\n",
    "    return np.array([[0.1], [0.3], [0.5]])\n",
    "\n",
    "def tx_testing():\n",
    "    return np.array([[2.3, 3.2], [1.0, 0.1], [1.4, 2.3]])\n",
    "\n",
    "def test_least_squares(y, tx):\n",
    "    w, loss = least_squares(y, tx)\n",
    "\n",
    "    expected_w = np.array([[0.218786], [-0.053837]])\n",
    "    expected_loss = 0.026942\n",
    "\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_ridge_regression_lambda0(y, tx):\n",
    "    lambda_ = 0.0\n",
    "    w, loss = ridge_regression(y, tx, lambda_)\n",
    "\n",
    "    expected_loss = 0.026942\n",
    "    expected_w = np.array([[0.218786], [-0.053837]])\n",
    "\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_ridge_regression_lambda1(y, tx):\n",
    "    lambda_ = 1.0\n",
    "    w, loss = ridge_regression(y, tx, lambda_)\n",
    "\n",
    "    expected_loss = 0.03175\n",
    "    expected_w = np.array([[0.054303], [0.042713]])\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_logistic_regression_0_step(y, tx):\n",
    "    expected_w = np.array([[0.463156], [0.939874]])\n",
    "    y = (y > 0.2) * 1.0\n",
    "    w, loss = logistic_regression(y, tx, expected_w, 0, GAMMA)\n",
    "\n",
    "    expected_loss = 1.533694\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_logistic_regression(y, tx, initial_w):\n",
    "    y = (y > 0.2) * 1.0\n",
    "    w, loss = logistic_regression(\n",
    "        y, tx, initial_w, MAX_ITERS, GAMMA\n",
    "    )\n",
    "\n",
    "    expected_loss = 1.348358\n",
    "    expected_w = np.array([[0.378561], [0.801131]])\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "    \n",
    "def test_reg_logistic_regression(y, tx, initial_w):\n",
    "    lambda_ = 1.0\n",
    "    y = (y > 0.2) * 1.0\n",
    "    w, loss = reg_logistic_regression(\n",
    "        y, tx, lambda_, initial_w, MAX_ITERS, GAMMA\n",
    "    )\n",
    "\n",
    "    expected_loss = 0.972165\n",
    "    expected_w = np.array([[0.216062], [0.467747]])\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape\n",
    "\n",
    "\n",
    "def test_reg_logistic_regression_0_step(y, tx):\n",
    "    lambda_ = 1.0\n",
    "    expected_w = np.array([[0.409111], [0.843996]])\n",
    "    y = (y > 0.2) * 1.0\n",
    "    w, loss = reg_logistic_regression(\n",
    "        y, tx, lambda_, expected_w, 0, GAMMA\n",
    "    )\n",
    "\n",
    "    expected_loss = 1.407327\n",
    "\n",
    "    np.testing.assert_allclose(loss, expected_loss, rtol=RTOL, atol=ATOL)\n",
    "    np.testing.assert_allclose(w, expected_w, rtol=RTOL, atol=ATOL)\n",
    "    assert loss.ndim == 0\n",
    "    assert w.shape == expected_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1717d4c",
   "metadata": {},
   "source": [
    "# Our Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb8ef1",
   "metadata": {},
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7677a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        w: weights, numpy array of shape(D,), D is the number of features.\n",
    "    \n",
    "    Returns:\n",
    "        mse: scalar corresponding to the mse with factor (1 / 2 n) in front of the sum\n",
    "\n",
    "    >>> compute_mse(np.array([0.1,0.2]), np.array([[2.3, 3.2], [1., 0.1]]), np.array([0.03947092, 0.00319628]))\n",
    "    0.006417022764962313\n",
    "    \"\"\"\n",
    "    \n",
    "    e = y - tx.dot(w)\n",
    "    \n",
    "    ## np.linalg.norm(e) ** 2 replaces e.dot(e)\n",
    "    mse = (np.linalg.norm(e) ** 2) / (2 * len(e))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ed2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"Calculate the least squares solution.\n",
    "       returns mse, and optimal weights.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        mse: scalar.\n",
    "    \"\"\"\n",
    "    # w = (np.linalg.inv(tx.transpose() @ tx)) @ (tx.transpose() @ y)\n",
    "    w = np.linalg.solve(tx.transpose() @ tx, tx.transpose() @ y)\n",
    "    mse = compute_mse(y, tx, w)\n",
    "    \n",
    "    return w, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67466bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_least_squares(y_testing(), tx_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0d90d",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of shape (N,), N is the number of samples.\n",
    "        tx: numpy array of shape (N,D), D is the number of features.\n",
    "        lambda_: scalar.\n",
    "    \n",
    "    Returns:\n",
    "        w: optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "    \"\"\"\n",
    "    D = 1 if len(tx.shape) == 1 else tx.shape[1]\n",
    "    N = len(y)\n",
    "    a = np.array((tx.transpose() @ tx) + (2 * N * lambda_) * np.eye(D))\n",
    "    b = np.array(tx.transpose() @ y)\n",
    "    \n",
    "    w = np.linalg.inv(a) @ b\n",
    "        \n",
    "    return w, compute_mse(y, tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0910a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ridge_regression_lambda0(y_testing(), tx_testing())\n",
    "test_ridge_regression_lambda1(y_testing(), tx_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827c388",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Passes the tests! ^^\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\n",
    "\n",
    "    Args:\n",
    "        t: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "        scalar or numpy array\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "\n",
    "    Returns:\n",
    "        a hessian matrix of shape=(D, D) \n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(y)\n",
    "    sig = sigmoid(tx @ w)\n",
    "    diag = sig * (1-sig) #np.zeros((N, N))\n",
    "    #np.fill_diagonal(diag,  sig * (1 - sig))\n",
    "                  \n",
    "    return (1 / N) * (tx.T @ (diag * tx))\n",
    "\n",
    "def calculate_loss(y, tx, w, penalty=0):\n",
    "    \"\"\"compute the cost by negative log likelihood.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "\n",
    "    Returns:\n",
    "        a non-negative loss\n",
    "    \"\"\"\n",
    "    assert y.shape[0]  == tx.shape[0]\n",
    "    assert tx.shape[1] == w.shape[0]\n",
    "    \n",
    "    sig = sigmoid(tx @ w) \n",
    "    left  = y * np.log(sig)\n",
    "    right = (1-y) * np.log(1 - sig)\n",
    "    \n",
    "    return - np.mean(left + right) + penalty\n",
    "\n",
    "def calculate_gradient(y, tx, w, lambda_=0):\n",
    "    \"\"\"compute the gradient of loss.\n",
    "    \n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1) \n",
    "\n",
    "    Returns:\n",
    "        a vector of shape (D, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    sig = sigmoid(tx @ w)\n",
    "    \n",
    "    ## last term is for adding a lambda_ * ||w||^2 penalty \n",
    "    return (1 / len(y)) * (tx.T @ (sig - y)) + (2 * lambda_ * w)\n",
    "\n",
    "## Uses gradient descent\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression with GD\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss = calculate_loss(y, tx, w) \n",
    "        w = w - gamma * calculate_gradient(y, tx, w)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    losses.append(calculate_loss(y, tx, w))\n",
    "    print(\"loss={l}\".format(l=losses[-1]))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dd653",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logistic_regression_0_step(y_testing(), tx_testing())\n",
    "test_logistic_regression(y_testing(), tx_testing(), initial_w_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0e102",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38baf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_penalty_term(lambda_, w):\n",
    "    return lambda_ * (np.linalg.norm(w) ** 2)\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression with GD\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss = calculate_loss(y, tx, w, penalty=compute_penalty_term(lambda_, w)) \n",
    "        w = w - gamma * calculate_gradient(y, tx, w, lambda_=lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    losses.append(calculate_loss(y, tx, w))\n",
    "    print(\"loss={l}\".format(l=losses[-1]))\n",
    "    \n",
    "    return w, losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a32bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reg_logistic_regression_0_step(y_testing(), tx_testing())\n",
    "test_reg_logistic_regression(y_testing(), tx_testing(), initial_w_testing())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c6520",
   "metadata": {},
   "source": [
    "# Loading Higgs Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee97ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"Split the dataset between train and test based on the split ratio.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n = len(y)\n",
    "    indices = np.random.permutation(n)\n",
    "    split = int(ratio * n)\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return x[train_indices], y[train_indices], x[test_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71237916",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test,  tx_test,  ids_test  = load_csv_data(\"D:/Downloads/test.csv\")\n",
    "y_train, tx_train, ids_train = load_csv_data(\"D:/Downloads/train.csv\")\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(tx_train, y_train, 0.8)\n",
    "\n",
    "N, D = x_train.shape\n",
    "\n",
    "print(f'Number of samples: {N}')\n",
    "print(f'Number of features: {D}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "y_test  = np.reshape(y_test, (len(y_test), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620faa4f",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0cbe8",
   "metadata": {},
   "source": [
    "## Feature Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3529711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(x):\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    return np.array([xi ** np.arange(2, degree+1) for xi in x])\n",
    "\n",
    "def append_poly_for_feature(x, feature_idx, degree):\n",
    "    x_feature = x[:, feature_idx]\n",
    "    polys = build_poly(x_feature, degree)\n",
    "    \n",
    "    return np.column_stack((x, polys))\n",
    "\n",
    "def expand_features(x, degree):\n",
    "    N, D = x.shape\n",
    "    for i in np.arange(D):\n",
    "        x = append_poly_for_feature(x, i, degree)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def expand_features_random_func(x, nb_features, func):\n",
    "    for i in np.arange(nb_features):\n",
    "        for j in np.arange(i+1, nb_features):\n",
    "            feature_i = x[:, i]\n",
    "            feature_j = x[:, j]\n",
    "            N = len(feature_i)\n",
    "            x = np.column_stack((x, np.array([func(feature_i[idx], feature_j[idx]) for idx in np.arange(N)])))\n",
    "    \n",
    "    return x\n",
    "\n",
    "def add_jet_binary_features(x, jet_idx):\n",
    "    jet_vals = x[:, jet_idx]\n",
    "    res1 = (jet_vals == 0).astype(int)\n",
    "    res2 = (jet_vals <= 1).astype(int)\n",
    "    res3 = (jet_vals >= 2).astype(int)\n",
    "    \n",
    "    for res in [res1, res2, res3]:\n",
    "        x = np.column_stack((x, res))\n",
    "        \n",
    "    return x\n",
    "\n",
    "def truncated_fourier_term(x, j):\n",
    "    assert j > 0\n",
    "    \n",
    "    return np.sin(j*np.pi*x) if j%2==0 else np.cos(j*np.pi*x)\n",
    "\n",
    "def append_fourier_for_feature(x, degree, feature_idx):\n",
    "    x_feature = x[:, feature_idx]\n",
    "    \n",
    "    for j in np.arange(1, degree+1):\n",
    "        x = np.column_stack((x, truncated_fourier_term(x_feature, j)))\n",
    "    \n",
    "    return x\n",
    "\n",
    "def append_fourier(x, degree, nb_features):\n",
    "    for i in np.arange(nb_features):\n",
    "        x = append_fourier_for_feature(x, degree, i)\n",
    "        \n",
    "    return x\n",
    "\n",
    "def my_sum(x, y):\n",
    "    return x + y\n",
    "\n",
    "def my_mult(x, y):\n",
    "    return x * y\n",
    "\n",
    "def my_combo(x, y):\n",
    "    return np.sin(x+y) + np.cos(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_expansion    = True\n",
    "jet_binary_features = False\n",
    "fourier_expansion   = False\n",
    "logs                = True\n",
    "my_mult_expansion   = True\n",
    "my_sum_expansion    = False\n",
    "my_combo_expansion  = True\n",
    "\n",
    "if(degree_expansion):\n",
    "    x_train = expand_features(x_train, 4)\n",
    "    x_test  = expand_features(x_test, 4)\n",
    "\n",
    "if(jet_binary_features):\n",
    "    x_train = add_jet_binary_features(x_train, 22)\n",
    "    x_test = add_jet_binary_features(x_test, 22)\n",
    "    \n",
    "if(fourier_expansion):\n",
    "    x_train = append_fourier(x_train, 4, 30)\n",
    "    x_test = append_fourier(x_test, 4, 30)\n",
    "    \n",
    "if(logs):\n",
    "    x_train = np.concatenate((x_train, np.log(x_train[:, [2, 9, 10, 13, 16, 19, 21]])), axis=1)\n",
    "    x_test = np.concatenate((x_test,   np.log(x_test[:, [2, 9, 10, 13, 16, 19, 21]])), axis=1)\n",
    "\n",
    "if(my_mult_expansion):\n",
    "    x_train = expand_features_random_func(x_train, 30, my_mult)\n",
    "    x_test = expand_features_random_func(x_test, 30, my_mult)\n",
    "    \n",
    "if(my_sum_expansion):\n",
    "    x_train = expand_features_random_func(x_train, 30, my_sum)\n",
    "    x_test = expand_features_random_func(x_test, 30, my_sum)\n",
    "    \n",
    "if(my_combo_expansion):\n",
    "    x_train = expand_features_random_func(x_train, 30, my_sum)\n",
    "    x_test = expand_features_random_func(x_test, 30, my_combo)\n",
    "\n",
    "#jets_train = x_train[:, 22]\n",
    "#jets_test  = x_test[:, 22]\n",
    "\n",
    "x_train = standardize_data(x_train)\n",
    "x_test  = standardize_data(x_test)\n",
    "\n",
    "#x_train[:, 22] = jets_train\n",
    "#x_test[:, 22]  = jets_test\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3fb2d",
   "metadata": {},
   "source": [
    "## Removing \"weird\" features (inactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c76100",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing weirdness score >= 60\n",
    "\n",
    "# Normalize each feature\n",
    "#tx_normalised = standardize_data(tx_train)\n",
    "#weirdness = np.sum(np.abs(tx_normalised), axis=1)\n",
    "#plt.boxplot(weirdness)\n",
    "#plt.show()\n",
    "\n",
    "#y_train = y_train[weirdness < 60]\n",
    "#x_train = x_train[weirdness < 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872a46e",
   "metadata": {},
   "source": [
    "## Our models: Starting with Least Squares, Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(truth, predictions):\n",
    "    return len(np.where(truth == predictions)[0]) / len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3bc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_prediction(prediction): \n",
    "    prediction[np.where(prediction>0)[0]]  = 1\n",
    "    prediction[np.where(prediction<=0)[0]] = -1\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_, mse_ = least_squares(y_train, x_train)\n",
    "\n",
    "prediction = x_test @ weights_\n",
    "\n",
    "transformed_prediction = transform_prediction(prediction)\n",
    "\n",
    "print(compute_accuracy(y_train, transform_prediction(x_train @ weights_)) * 100)\n",
    "\n",
    "print(compute_accuracy(y_test, transformed_prediction) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lambda = 5e-4 71.6ish%\n",
    "\n",
    "weights_, mse_ = ridge_regression(y_train, x_train,1.7)\n",
    "\n",
    "prediction = x_test @ weights_\n",
    "\n",
    "transformed_prediction = transform_prediction(prediction)\n",
    "\n",
    "print(compute_accuracy(y_train, transform_prediction(x_train @ weights_)) * 100)\n",
    "\n",
    "print(compute_accuracy(y_test, transformed_prediction) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uses gradient descent\n",
    "def logistic_regression_newton(y, tx, initial_w, max_iters, gamma):\n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    w = initial_w\n",
    "\n",
    "    # start the logistic regression with GD\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss = calculate_loss(y, tx, w) \n",
    "        hess = calculate_hessian(y, tx, w)\n",
    "        w = np.linalg.solve(hess, hess @ w - gamma * calculate_gradient(y, tx, w))\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    losses.append(calculate_loss(y, tx, w))\n",
    "    print(\"loss={l}\".format(l=losses[-1]))\n",
    "    \n",
    "    return w, losses[-1]\n",
    "\n",
    "## lambda = 5e-4 71.6ish%\n",
    "\n",
    "N, D = x_train.shape\n",
    "\n",
    "weights_, mse_ = logistic_regression_newton(y_train.reshape((N, 1)), x_train, np.zeros((D, 1)), 400, 5e-4)\n",
    "\n",
    "prediction = x_test @ weights_\n",
    "\n",
    "transformed_prediction = transform_prediction(prediction)\n",
    "\n",
    "print(compute_accuracy(y_train.reshape((N, 1)), transform_prediction(x_train @ weights_)) * 100)\n",
    "\n",
    "print(compute_accuracy(y_test.reshape((len(y_test), 1)), transformed_prediction) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881d34d",
   "metadata": {},
   "source": [
    "## K-fold cross validation on lambda for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, gamma=0.1):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "    \n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,D)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "    \n",
    "    N, D = x.shape\n",
    "\n",
    "    # ***************************************************\n",
    "    # get k'th subgroup in test, others in train\n",
    "    # ***************************************************\n",
    "    k_te_indices = k_indices[k]\n",
    "    te_mask = np.zeros(N, dtype = bool)\n",
    "    te_mask[k_te_indices] = True\n",
    "    \n",
    "    y_te = y[te_mask]\n",
    "    y_tr = y[~te_mask]\n",
    "    \n",
    "    x_te = x[te_mask]\n",
    "    x_tr = x[~te_mask]\n",
    "    \n",
    "    # ***************************************************\n",
    "    # Regularized logistic regression\n",
    "    # ***************************************************\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    initial_weights = np.random.randn(D, 1)\n",
    "    max_iters = 150\n",
    "    weights, loss = reg_logistic_regression(y_tr, x_tr, lambda_, initial_weights, max_iters, gamma)\n",
    "    \n",
    "    # ***************************************************\n",
    "    # calculate the loss for train and test data\n",
    "    # ***************************************************\n",
    "    \n",
    "    loss_tr = np.sqrt(2 * compute_mse(y_tr, x_tr, weights))\n",
    "    loss_te = np.sqrt(2 * compute_mse(y_te, x_te, weights))\n",
    "    \n",
    "    return loss_tr, loss_te\n",
    "\n",
    "def cross_validation_demo(y, x, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "    \n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    seed = 12\n",
    "    k_fold = k_fold\n",
    "    lambdas = lambdas\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # ***************************************************\n",
    "    # cross validation over lambdas\n",
    "    # ***************************************************\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        aux_tr = 0; aux_te = 0\n",
    "        for k in np.arange(k_fold):\n",
    "            loss_tr_tmp, loss_te_tmp = cross_validation(y, x, k_indices, k, lambda_)\n",
    "            \n",
    "            aux_tr += loss_tr_tmp\n",
    "            aux_te += loss_te_tmp\n",
    "            \n",
    "        rmse_tr.append(aux_tr/k_fold)\n",
    "        rmse_te.append(aux_te/k_fold)   \n",
    "\n",
    "    ## Computing the best lambda & test rmse tuple\n",
    "    best_idx = np.argmin(rmse_te)\n",
    "    \n",
    "    best_lambda = lambdas[best_idx]\n",
    "    best_rmse   = rmse_te[best_idx]\n",
    "        \n",
    "    print(\"The choice of lambda which leads to the best test rmse is %.5f with a test rmse of %.3f\" % (best_lambda, best_rmse))\n",
    "    return best_lambda, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e937e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "gamma = 0.1\n",
    "lambda_ = 0.5\n",
    "\n",
    "initial_weight = np.random.randn(D,1)\n",
    "\n",
    "weights, loss = reg_logistic_regression(y_train.reshape((len(y_train), 1)), x_train, lambda_, initial_weight, 900, gamma)\n",
    "\n",
    "print(f'After cross validation on the choice of lambda_, we obtain a loss of {calculate_loss(y_test.reshape((len(y_test), 1)), x_test, weights)} on the test set.')\n",
    "\n",
    "\n",
    "prediction = x_test @ weights\n",
    "\n",
    "transformed_prediction = transform_prediction(prediction)\n",
    "\n",
    "print(compute_accuracy(y_train.reshape((len(y_train), 1)), transform_prediction(x_train @ weights)) * 100)\n",
    "\n",
    "print(compute_accuracy(y_test.reshape((len(y_test), 1)), transformed_prediction) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fn call\n",
    "best_gamma, best_lambda, best_rmse = best_param_selection(y_train, tx_train, np.logspace(-4, 0, 30), 4, np.logspace(-4, 0, 30))\n",
    "\n",
    "print(f'[Reg. Logistic Regression] In max_iter={max_iter}, with hyperparameters lambda={lambda_} and gamma={gamma} we obtain a loss={loss} and a test loss={5} TODO')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
